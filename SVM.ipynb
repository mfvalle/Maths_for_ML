{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfvalle/Maths_for_MachineLearning/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdbEqoIXQosC"
      },
      "source": [
        "# Entrega 1\n",
        "\n",
        "Sergio Quiroga Sandoval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducibility\n",
        "we will provide a reproducible notebook with automatic download data from a github repository created for this project\n",
        "\n"
      ],
      "metadata": {
        "id": "diw-XAk7X_Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important: run the following command"
      ],
      "metadata": {
        "id": "i2eeqip0GOa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!git clone https://github.com/SergioQS/Machine-Learning.git  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9fDsE1GDXaN",
        "outputId": "dfaccc35-009b-4f75-95ab-294d46abf6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine-Learning  sample_data\n",
            "fatal: destination path 'Machine-Learning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see sample_data then you can run the following code ONLY ONCE\n"
      ],
      "metadata": {
        "id": "tIo-ZwDdG5M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def hms_to_s(s):                # funcion para pasar horas:minutos:segundos a minutos\n",
        "    t = 0\n",
        "    for u in s.split(':'):\n",
        "        t = 60 * t + int(u)\n",
        "    t = t/60\n",
        "    return t"
      ],
      "metadata": {
        "id": "ZL1_KMSSYPXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izybe26rQosF"
      },
      "source": [
        "## Tasks\n",
        "for both problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkHh54QxQosG"
      },
      "source": [
        "1) Specify which Machine Learning problem are you solving\n",
        "\n",
        "2) Provide a short summary of the features and the labels you are working on\n",
        "\n",
        "3) Please answer the following questions: \n",
        "\n",
        "    a) Are these datasets linearly separable?\n",
        "    b) Are these datasets randomly chosen \n",
        "    c) The sample size is enough to guarantee generalization\n",
        "\n",
        "4) Provide an explanation how and why the code is working\n",
        "\n",
        "5) Show some examples to illustrate that the method is working properly............  (evaluarlo para ejemplos del data test)\n",
        "\n",
        "6) Provide quantitative evidence for generalization using the provided dataset....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0szVMiCiQosH"
      },
      "source": [
        "## First dataset: occupancy rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TlQ5sBdQosH"
      },
      "source": [
        "### 1) Machine Learning problem "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pre04cM5QosH"
      },
      "source": [
        "We are solving a binary classification problem using support vector machines, first we have a dataset consisting of \n",
        "7 dimensional vectors with the information of \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"\n",
        "\n",
        "The goal is implementing a support vector machines model for predicting the \"Occupancy\" given the other 6 data.\n",
        "For this we will check if the dataset is linearly separable, for correctly implementing support vector machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SibM26fuQosI"
      },
      "source": [
        "### 2) short summary of the features and the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pnJitdFQosI"
      },
      "source": [
        "Our features and format are the following\n",
        "\n",
        "\n",
        "\n",
        "*   \"date\": date time year-month-day hour:minute:second\n",
        "*   \"Light\": (light in lux)\n",
        "\n",
        "\n",
        "\"Temperature\": in celcius\n",
        "\"humidity\": (Relative Humidity, %)\n",
        "\"co2\":(CO2, in ppm)\n",
        "\"HumidityRatio\":(Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air)\n",
        "\n",
        "\n",
        "lets see if there is a relation between each feature and the \"Occupancy\". \n",
        "it is important because we need to be sure we use adecuate features on the dataset for predicting occupancy. \n",
        "\n",
        "\"date\": important data because it can be related with the normal values of \"Temperature\",\"Humidity\" and \"Light\",\n",
        "because these may vary during the day so we will consider for our model the hour in the day where the 7 data was recorded.\n",
        "\"Temperature\": temperatue is related with occupancy because humans breath can increase temperature in a room.\n",
        "\n",
        "\"Humidity\": humidity increases with the number of persons in a room, so it is related with occupancy.\n",
        "\n",
        "\"Light\" : an occupied room may have the light turned on\".\n",
        "\n",
        "\"CO2\":  the levels of CO2 increase with the more people there are in a room.\n",
        "\n",
        "\"HumidityRatio\": \n",
        "\n",
        "Occupancy, 0 or 1, 0 for not occupied, 1 for occupied status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyu4ih9LQosJ"
      },
      "source": [
        "### 3) a. Are these datasets linearly separable?\n",
        "for checking data is linearly separable, lets see if an svm can overfit perfectly the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing data\n",
        "datatraining_text = pd.read_csv('/content/Machine-Learning/datatraining.csv', encoding='utf-8')\n",
        "\n",
        "print(datatraining_text.head)\n",
        "print(type(datatraining_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtHt6jwm1Dl6",
        "outputId": "adb61a56-c234-4ec9-b363-04444ae1ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of      \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"\n",
            "0     \"1\",\"2015-02-04 17:51:00\",23.18,27.272,426,721...                       \n",
            "1     \"2\",\"2015-02-04 17:51:59\",23.15,27.2675,429.5,...                       \n",
            "2     \"3\",\"2015-02-04 17:53:00\",23.15,27.245,426,713...                       \n",
            "3     \"4\",\"2015-02-04 17:54:00\",23.15,27.2,426,708.2...                       \n",
            "4     \"5\",\"2015-02-04 17:55:00\",23.1,27.2,426,704.5,...                       \n",
            "...                                                 ...                       \n",
            "8138  \"8139\",\"2015-02-10 09:29:00\",21.05,36.0975,433...                       \n",
            "8139  \"8140\",\"2015-02-10 09:29:59\",21.05,35.995,433,...                       \n",
            "8140  \"8141\",\"2015-02-10 09:30:59\",21.1,36.095,433,7...                       \n",
            "8141  \"8142\",\"2015-02-10 09:32:00\",21.1,36.26,433,82...                       \n",
            "8142  \"8143\",\"2015-02-10 09:33:00\",21.1,36.2,447,821...                       \n",
            "\n",
            "[8143 rows x 1 columns]>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKCzApPWQosK",
        "outputId": "498ddb00-33ef-4e32-8827-61efc0ac1e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1071.0, 23.18, 27.272, 426.0, 721.25, 0.00479298817650529], [1071.9833333333333, 23.15, 27.2675, 429.5, 714.0, 0.00478344094931065], [1073.0, 23.15, 27.245, 426.0, 713.5, 0.00477946352442199], [1074.0, 23.15, 27.2, 426.0, 708.25, 0.00477150882608175], [1075.0, 23.1, 27.2, 426.0, 704.5, 0.00475699293331518], [1075.9833333333333, 23.1, 27.2, 419.0, 701.0, 0.00475699293331518], [1077.0, 23.1, 27.2, 419.0, 701.666666666667, 0.00475699293331518], [1077.9833333333333, 23.1, 27.2, 419.0, 699.0, 0.00475699293331518], [1078.9833333333333, 23.1, 27.2, 419.0, 689.333333333333, 0.00475699293331518], [1080.0, 23.075, 27.175, 419.0, 688.0, 0.00474535071966655]]\n",
            "lenghts of the classes are:  6414 and:  1729\n",
            "lenght of X is:  8143\n",
            "lenght of y is:  8143\n",
            "el modelo no predice bien algun/algunos ejemplos del dataset de training\n",
            "False\n",
            "los siguientes son los indices de los elementos con predicción erronea [811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1749, 1750, 1751, 1752, 1753, 1754, 2689, 2692, 5513, 5514, 5515, 6412, 6413, 6968, 7271, 7818]\n"
          ]
        }
      ],
      "source": [
        "# preparing the dataset \n",
        "data_dict = np.array(datatraining_text)\n",
        "for i in range(len(data_dict)):\n",
        "    data_dict[i][0] = (data_dict[i][0].split(\",\"))\n",
        "\n",
        "\n",
        "class0 = []\n",
        "class1 = []\n",
        "fechas = []\n",
        "\n",
        "\"\"\"ingeniería de características. usando solo la hora del día del dato de la fecha, porque puede tener incidencia en niveles de humedad base \n",
        "durante el dia\"\"\"\n",
        "\n",
        "for i in range(len(data_dict)):\n",
        "    del data_dict[i][0][0]                    # borrar la numeración\n",
        "    fechas.append(data_dict[i][0][0])         # crear un array con la fecha de cada vector de datos\n",
        "    fechas[i] = fechas[i].split( )            # manejo y limpieza de la fecha\n",
        "    del fechas[i][0]\n",
        "    fechas[i] = ''.join(fechas[i])\n",
        "    fechas[i] = fechas[i].replace('\"', '')\n",
        "    fechas[i] = hms_to_s(fechas[i])\n",
        "    data_dict[i][0][0] = fechas[i]                          # reincorporar hora convertida solo a minutos en el diccionario\n",
        "    data_dict[i][0] = [float(x) for x in data_dict[i][0]]   # convertir las entradas de cada array a float type\n",
        "    if data_dict[i][0][6] == 0:                             # separar el dataset en las dos clases posibles dependiendo del output\n",
        "        class0.append(data_dict[i][0])\n",
        "    else:\n",
        "        class1.append(data_dict[i][0])\n",
        "\n",
        "for i in range(len(class0)):\n",
        "   del class0[i][-1]\n",
        "for i in range(len(class1)):\n",
        "   del class1[i][-1]\n",
        "\n",
        "print(class1[0:10])\n",
        "\n",
        "\"\"\" el dataset está separado en dos clases, ahora hay que crear un vector de etiquetas de la forma y = [1,1,1,1,...,1,-1,-1,-1,...,-1] \n",
        " donde el numero de 1 sea len(class0) y el numero de -1 sea len(class1) y luego unir las dos clases en un solo vector de \n",
        " caracteríaticas (X = [...]) (esto se hace para poder meter el dataset en sklearn.svm)\n",
        "\"\"\"\n",
        "y = []\n",
        "\n",
        "print(\"lenghts of the classes are: \", len(class0),\"and: \", len(class1))\n",
        "\n",
        "for i in range(len(class0)):\n",
        "    y.append(1)\n",
        "for i in range(len(class1)):\n",
        "    y.append(-1)\n",
        "\n",
        "\n",
        "X = class0 + class1\n",
        "\n",
        "\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "print( \"lenght of X is: \", len(X))  \n",
        "\n",
        "print( \"lenght of y is: \", len(y))   # X y y ya tienen la misma longitud.\n",
        "\n",
        "\n",
        "# implementación del svm:\n",
        "\n",
        "\"\"\"\n",
        "para implementar la funcion clf.fit(X, y) los datos deben tener esta forma:\n",
        "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
        "y = np.array([1, 1, 2, 2])\n",
        "\"\"\"\n",
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X, y)\n",
        "\n",
        "predict_X = clf.predict(X)\n",
        "\n",
        "#if predict_X.all() == y.all():\n",
        "  \n",
        "if np.array_equiv(predict_X, y):\n",
        "    print(\"el modelo predice perfectamente todos los x del entrenamiento (overfit), por lo tanto el dataset es linealmente separable\")\n",
        "else:\n",
        "    print(\"el modelo no predice bien algun/algunos ejemplos del dataset de training\")\n",
        "\n",
        "print((predict_X == y).all(),)\n",
        "\n",
        "#el modelo se está equivocando en los siguientes datos del training set. \n",
        "indices_dif = []\n",
        "for i in range(len(y)):\n",
        "  if predict_X[i] != y[i]:\n",
        "    indices_dif.append(i)\n",
        "\n",
        "print(\"los siguientes son los indices de los elementos con predicción erronea\",indices_dif)\n",
        "#print(\"la matriz de confusion se ve de la siguiente forma (ya normalizada con el valor true): \\n\", confusion_matrix(y, predict_X,normalize=\"true\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) b. Are these datasets randomly chosen and \n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u-IVfgFcTE9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make an analysis of the possible ways in wich the data was collected.\n",
        "\n",
        "since data is from occupancy of rooms it is possible that there are some biases in the collecting of data like the following ones:\n",
        "*   the season in wich data was recorded.\n",
        "*   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8y2qKYfiuozt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) c. The sample size is enough to guarantee generalization.\n",
        "\n",
        "Taking into account the given data of 8143 and that there are 6 features per data. \n",
        "We have a 6 dimensional parameter space, so we see that the amount of available data is larger than ten times the dimensions of the dataset.\n",
        "for that reason we could say that there is enough data to generalize"
      ],
      "metadata": {
        "id": "opwkTmlFTajT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Provide an explanation how and why the code is working\n",
        "\n",
        "the initial code wasnt working because:\n",
        "\n",
        "*   the directionalvectors were smaller than expected by the size of the data in occupancy dataset.\n",
        "*   also it was designed for a two dimensional problem, so it had plots implemented for visualising the dataset.\n",
        "*   the step size was not optimum.\n",
        "*   the algorithm was not reading correctly the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For this reason we used the scikit implementation of SVM."
      ],
      "metadata": {
        "id": "2F5QCUmjUFtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Show some examples to illustrate that the method is working properly."
      ],
      "metadata": {
        "id": "Sjlemd-xTfVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qPJpT8nq0nKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lets see some examples of the model working: \", clf.predict(X[0:5]), y[0:5])\n",
        "\n",
        "print(clf.predict(X[60:65]), y[60:65])\n",
        "\n",
        "print(clf.predict(X[4060:4065]), y[4060:4065])\n",
        "\n",
        "print(clf.predict(X[7060:7065]), y[7060:7065])\n",
        "\n",
        "\n",
        "print(clf.predict(X[6060:6065]), y[6060:6065])\n",
        "\n",
        "\n",
        "print(clf.predict(X[8060:8065]), y[8060:8065])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Rji9vsMJ0w",
        "outputId": "298521a9-ac14-4ab9-9cac-2ab9c7d9d2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lets see some examples of the model working:  [1 1 1 1 1] [1 1 1 1 1]\n",
            "[1 1 1 1 1] [1 1 1 1 1]\n",
            "[1 1 1 1 1] [1 1 1 1 1]\n",
            "[-1 -1 -1 -1 -1] [-1 -1 -1 -1 -1]\n",
            "[1 1 1 1 1] [1 1 1 1 1]\n",
            "[-1 -1 -1 -1 -1] [-1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Provide quantitative evidence for generalization using the provided dataset"
      ],
      "metadata": {
        "id": "92d-rCRf0aTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# punto 6\n",
        "# veamos si el modelo generaliza  \n",
        "\"\"\" lo que haremos es separar el dataset en dos conjuntos x_test, x_train, y_test, y_train \n",
        "luego entrenamos el modelo con el conjunto train y lo evaluamos en los ejemplos del conjunto test\n",
        "\"\"\"\n",
        "data_test = pd.read_csv(\"/content/Machine-Learning/datatest.csv\", encoding='utf-8') \n",
        "# le haremos el mismo proceso anterior al datatest\n",
        "\n",
        "data_dict_test = np.array(data_test)\n",
        "\n",
        "\n",
        "for i in range(len(data_dict_test)):\n",
        "    data_dict_test[i][0] = (data_dict_test[i][0].split(\",\"))\n",
        "\n",
        "\n",
        "class0_test = []\n",
        "class1_test = []\n",
        "#ingeniería de características. usando solo la hora del día del dato de la fecha, porque puede tener incidencia en niveles de humedad base\n",
        "fechas_test = []\n",
        "\n",
        "for i in range(len(data_dict_test)):\n",
        "    del data_dict_test[i][0][0]                    # borrar la numeración\n",
        "    fechas_test.append(data_dict_test[i][0][0])         #crear un array con la fecha de cada vector de datos\n",
        "    fechas_test[i] = fechas_test[i].split( )            # manejo y limpieza de la fecha\n",
        "    del fechas_test[i][0]\n",
        "    fechas_test[i] = ''.join(fechas_test[i])\n",
        "    fechas_test[i] = fechas_test[i].replace('\"', '')\n",
        "    fechas_test[i] = hms_to_s(fechas_test[i])\n",
        "    data_dict_test[i][0][0] = fechas_test[i]            # reincorporar hora convertida solo a minutos en el diccionario\n",
        "\n",
        "    if data_dict_test[i][0][6] == \"0\":             # separar el dataset en las dos clases posibles dependiendo del output\n",
        "        class0_test.append(data_dict_test[i][0])\n",
        "    else:\n",
        "        class1_test.append(data_dict_test[i][0])\n",
        "\n",
        "for i in range(len(class0_test)):\n",
        "   del class0_test[i][-1]\n",
        "for i in range(len(class1_test)):\n",
        "   del class1_test[i][-1]\n",
        "for i in range(len(class0_test)):\n",
        "    for j in range(len(class0_test[i])):\n",
        "        class0_test[i][j] = float(class0_test[i][j])\n",
        "for i in range(len(class1_test)):\n",
        "    for j in range(len(class1_test[i])):\n",
        "        class1_test[i][j] = float(class1_test[i][j])\n",
        "\n",
        "#print(class1_test[0:10])\n",
        "\n",
        "y_test = []\n",
        "for i in range(len(class0_test)):\n",
        "    y_test.append(1)\n",
        "for i in range(len(class1_test)):\n",
        "    y_test.append(-1)\n",
        "X_test = class0_test + class1_test\n",
        "y_test = np.array(y_test)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "print( \"lenght of X_test is: \", len(X_test))  \n",
        "print( \"lenght of y_test is: \", len(y_test))   \n",
        "\n",
        "#entrenando el modelo:\n",
        "#clf.fit(x_train, y_train) ya se corrió antes\n",
        "predict = clf.predict(X_test)\n",
        "\n",
        "indices_diferentes = []\n",
        "n=0\n",
        "while n < len(predict): \n",
        "    if predict[n] != y_test[n]:\n",
        "        indices_diferentes.append(n)\n",
        "    n = n+1\n",
        "print(\"el modelo se equivoca en los siguientes indices de datos\", indices_diferentes)\n",
        "\n",
        "print('Accuracy of the model in the Test set is:', clf.score(X_test, y_test))\n",
        "\n",
        "#-----------------\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(y_test, predict)\n",
        "recall = recall_score(y_test, predict)\n",
        "f1 = f1_score(y_test, predict)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "#-----------------\n",
        "if recall_score(y_test, predict) > 0.95:\n",
        "    print(\"luego el modelo es capaz de generalizar en datos no vistos anteriormente, pues la clase class_0_test tiene: \", len(class0_test), \" elementos y class_1_test tiene: \", len(class1_test), \"por lo que es adecuado hablar de recall\")\n",
        "\n",
        "\n",
        "#print('Predicted Values from Classifier:', predict)\n",
        "#print('Actual Output is:', y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tva4VnOUm6IZ",
        "outputId": "bf57a3e9-ecf4-48f4-ae6e-d1a51becc6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght of X_test is:  2665\n",
            "lenght of y_test is:  2665\n",
            "el modelo se equivoca en los siguientes indices de datos [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 866, 867, 868, 869, 870, 871, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1896, 2601, 2602]\n",
            "Accuracy of the model in the Test set is: 0.9778611632270169\n",
            "Precision: 0.9981707317073171\n",
            "Recall: 0.9669226225634967\n",
            "F1 Score: 0.9822982298229822\n",
            "luego el modelo es capaz de generalizar en datos no vistos anteriormente, pues la clase class_0_test tiene:  1693  elementos y class_1_test tiene:  972 por lo que es adecuado hablar de recall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X_test, y_test, cv=5)\n",
        "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPTLmrwUOcAh",
        "outputId": "e662609e-5f25-40d2-ba3f-4aa6a8b69e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.96 accuracy with a standard deviation of 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Dataset: Banknote Autentication"
      ],
      "metadata": {
        "id": "cYBdEliTVNYa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKw1Nl1bVgcF"
      },
      "source": [
        "### 1) Machine Learning problem "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem we are solving is creating a binary classification model into two classes (real, counterfeited) for banknote authentication using Support Vector Machines.\n",
        "\n",
        "\n",
        "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
        "\n"
      ],
      "metadata": {
        "id": "7u_ap0ZEZrxH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyC4crlTViQ_"
      },
      "source": [
        "### 2) short summary of the features and the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has vector parameters with 4 dimensions and the class corresponding to them.\n",
        "\n",
        "The first 3 parameters refer to the corresponding moments of the wavelet transformed image.\n",
        "1. variance of Wavelet Transformed image (continuous)\n",
        "2. skewness of Wavelet Transformed image (continuous)\n",
        "3. curtosis of Wavelet Transformed image (continuous)\n",
        "4. entropy of image (continuous)\n",
        "5. class (integer)"
      ],
      "metadata": {
        "id": "IQKOPCE_v6ix"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6vmCMCgVpnF"
      },
      "source": [
        "### 3) a. Are these datasets linearly separable?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For checking linear separability we will try to completely overfit the dataset using a hard margin svm implementation"
      ],
      "metadata": {
        "id": "PukhHfgvwhuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datatraining_text2 = pd.read_csv('/content/Machine-Learning/data_banknote_authentication.csv', encoding='utf-8')\n",
        "\n",
        "print(datatraining_text2.head)\n",
        "print(type(datatraining_text2))"
      ],
      "metadata": {
        "id": "JjTewlQmwgx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39798e51-ff13-4e3c-8cb2-e414fea64530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         3.6216,8.6661,-2.8073,-0.44699,0\n",
            "0        4.5459,8.1674,-2.4586,-1.4621,0\n",
            "1         3.866,-2.6383,1.9242,0.10645,0\n",
            "2        3.4566,9.5228,-4.0112,-3.5944,0\n",
            "3       0.32924,-4.4552,4.5718,-0.9888,0\n",
            "4        4.3684,9.6718,-3.9606,-3.1625,0\n",
            "...                                  ...\n",
            "1366   0.40614,1.3492,-1.4501,-0.55949,1\n",
            "1367    -1.3887,-4.8773,6.4774,0.34179,1\n",
            "1368  -3.7503,-13.4586,17.5932,-2.7771,1\n",
            "1369    -3.5637,-8.3827,12.393,-1.2823,1\n",
            "1370    -2.5419,-0.65804,2.6842,1.1952,1\n",
            "\n",
            "[1371 rows x 1 columns]>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data_dict2 = np.array(datatraining_text2)\n",
        "for i in range(len(data_dict2)):\n",
        "    data_dict2[i][0] = (data_dict2[i][0].split(\",\"))\n",
        "\n",
        "\n",
        "class1_2 = []\n",
        "class0_2 = []\n",
        "for i in range(len(data_dict2)):   \n",
        "    data_dict2[i][0] = [float(x) for x in data_dict2[i][0]]              \n",
        "    if data_dict2[i][0][4] == 0:             # separar el dataset en las dos clases posibles dependiendo del output\n",
        "        data_dict2[i][0] = [float(x) for x in data_dict2[i][0]]\n",
        "        class0_2.append(data_dict2[i][0])\n",
        "    else:\n",
        "        class1_2.append(data_dict2[i][0])\n",
        "\n",
        "for i in range(len(class0_2)):\n",
        "   del class0_2[i][-1]\n",
        "for i in range(len(class1_2)):\n",
        "   del class1_2[i][-1]\n",
        "\n",
        "\n",
        "#print(class1_2[0:10])\n",
        "#print(class0_2[0:10])\n",
        "y_2 = []\n",
        "\n",
        "print(\"lenghts of the classes are: \", len(class0_2),\"and: \", len(class1_2))\n",
        "\n",
        "for i in range(len(class0_2)):\n",
        "    y_2.append(1)\n",
        "for i in range(len(class1_2)):\n",
        "    y_2.append(-1)\n",
        "\n",
        "\n",
        "X_2 = class0_2 + class1_2\n",
        "\n",
        "\n",
        "y_2 = np.array(y_2)\n",
        "X_2 = np.array(X_2)\n",
        "\n",
        "print( \"lenght of X is: \", len(X_2))  \n",
        "\n",
        "print( \"lenght of y is: \", len(y_2)) \n",
        "\n",
        "\n",
        "clf_2 = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf_2.fit(X_2, y_2)\n",
        "\n",
        "predict_X_2 = clf_2.predict(X_2)\n",
        "\n",
        "\n",
        "\n",
        "if np.array_equiv(predict_X_2, y_2):\n",
        "    print(\"el modelo predice perfectamente todos los x del entrenamiento (overfit), por lo tanto el dataset es linealmente separable\")\n",
        "else:\n",
        "    print(\"el modelo no predice bien algun/algunos ejemplos del dataset\")\n",
        "\n",
        "print((predict_X_2 == y_2).all(),)\n",
        "\n",
        "indices_dif_2 = []\n",
        "for i in range(len(y_2)):\n",
        "  if predict_X_2[i] != y_2[i]:\n",
        "    indices_dif_2.append(i)\n",
        "\n",
        "print(\"los siguientes son los indices de los elementos con predicción erronea\",indices_dif_2)"
      ],
      "metadata": {
        "id": "TPNBUk7N5AxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9343745-117e-41fb-c9a0-0bc19404e872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenghts of the classes are:  761 and:  610\n",
            "lenght of X is:  1371\n",
            "lenght of y is:  1371\n",
            "el modelo predice perfectamente todos los x del entrenamiento (overfit), por lo tanto el dataset es linealmente separable\n",
            "True\n",
            "los siguientes son los indices de los elementos con predicción erronea []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) b. Are these datasets randomly chosen \n",
        "\n",
        "these are some of the possible biases during the collection of data\n",
        "\n",
        "*   The counterfeited bills may have been from the same forger and it is possible that the model will not generalize to other forger.\n",
        "*   The genuine bills may be in a bad state so they may be misclassified.\n",
        "\n",
        "\n",
        "So the collection of data is not guaranteed to be randomly chosen.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qm3IGNJzVxDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) c. The sample size is enough to guarantee generalization.\n",
        "\n",
        "To guarantee generalization we need at least around 10 times the VC dimension, so it would me around 500 data, our dataset contains 1371 data, so we could say it guarantee generalization.\n"
      ],
      "metadata": {
        "id": "aRgIEGn5V3I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Provide an explanation how and why the code is working\n",
        "\n",
        "the initial code wasnt working because:\n",
        "\n",
        "*   the directionalvectors were smaller than expected by the size of the data in occupancy dataset.\n",
        "*   also it was designed for a two dimensional problem, so it had plots implemented for visualising the dataset.\n",
        "*   the step size was not optimum.\n",
        "*   the algorithm was not reading correctly the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For this reason we used the scikit implementation of SVM."
      ],
      "metadata": {
        "id": "ALiGcc6-V63V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Provide quantitative evidence for generalization using the provided dataset"
      ],
      "metadata": {
        "id": "fwY6026TEIfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "as a first attempt to have an insight on the generalization of the model we will make a 60/40 partition for train/test in the original dataset.\n",
        "then in the next block of code we will implement a cross validation testing"
      ],
      "metadata": {
        "id": "qroBAaGuLwk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_2[0:3])\n",
        "print(y_2[0:3])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.4, random_state=0)\n",
        "\n",
        "\n",
        "clf_general = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf_general.fit(X_train, y_train)\n",
        "\n",
        "predict_X_test = clf_general.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"the score of the model in testing is: \",clf_general.score(X_test, y_test))\n",
        "\n",
        "\n",
        "errors = []\n",
        "for i in range(len(y_test)):\n",
        "  if predict_X_test[i] != y_test[i]:\n",
        "    errors.append(i)\n",
        "print(\"los siguientes son los indices de los elementos con predicción erronea en el testing: \", errors)\n",
        "print(\"el siguiente valor de verdad indica si la prediccion es exactamente igual al y_test: \",(predict_X_test == y_test).all())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKm4DjxLETTn",
        "outputId": "fc087dd1-bbe6-4c6f-84da-d6cc16a6a283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.5459   8.1674  -2.4586  -1.4621 ]\n",
            " [ 3.866   -2.6383   1.9242   0.10645]\n",
            " [ 3.4566   9.5228  -4.0112  -3.5944 ]]\n",
            "[1 1 1]\n",
            "the score of the model in testing is:  1.0\n",
            "los siguientes son los indices de los elementos con predicción erronea en el testing:  []\n",
            "el siguiente valor de verdad indica si la prediccion es exactamente igual al y_test:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take a look on the following lines of scikit docs: \n",
        "\"When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\"\n",
        "\n",
        "this is happening, so we will implement a cross validation "
      ],
      "metadata": {
        "id": "27mQnB5sMjZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf_general, X_test, y_test, cv=5)\n",
        "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(y_test, predict_X_test)\n",
        "recall = recall_score(y_test, predict_X_test)\n",
        "f1 = f1_score(y_test, predict_X_test)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZIfT8DVMD3L",
        "outputId": "dbd3c864-1f8f-4f76-a06a-d2df18c02bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.99 accuracy with a standard deviation of 0.01\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Show some examples to illustrate that the method is working properly."
      ],
      "metadata": {
        "id": "n8P-aVEBV9by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lets see some examples of the model working: \", (predict_X_test[0:5]), y_test[0:5])\n",
        "\n",
        "print((predict_X_test[60:65]), y_test[60:65])\n",
        "print((predict_X_test[500:505]), y_test[500:505])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3jnUxVfdIRp",
        "outputId": "5f173dba-2c53-4efb-f067-8f5816e1a0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lets see some examples of the model working:  [-1  1 -1  1  1] [-1  1 -1  1  1]\n",
            "[-1  1  1  1  1] [-1  1  1  1  1]\n",
            "[ 1  1 -1 -1  1] [ 1  1 -1 -1  1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}